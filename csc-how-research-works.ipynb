{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for *How Research Works: An \"Under the Hood\" Exploration of Search Algorithms*\n",
    "\n",
    "This is the companion code for a workshop done at the Vagelos Computational Science Center at Barnard College by Dan Woulfin and Sydni Meyer. \n",
    "\n",
    "It's split into two parts:\n",
    "- A functional breakdown of boolean search\n",
    "- An exploration of three information retrieval ranking/relevance algorithms: Jaccard similarity, TF-IDF, and BM25\n",
    "\n",
    "We will be using a generated collection of documents as our data source. It's about Renaissance painters, the Teenage Mutant Ninja Turtles, and biological turtles. Please note that this is generated text and not verified facts about these topics. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean search \n",
    "\n",
    "Boolean search is direct search meaning that it returns a set of matching documents. It does so through set theory, testing each boolean operation according to an order of operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed libraries\n",
    "\n",
    "To keep this simple, we'll only be importing `re` to process regular expressions and pandas to create dataframes in the relevance section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean search functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query tokenization and preprocessing functions\n",
    "\n",
    "These functions will tokenize the query and preprocess them to add an implicit OR when none exist and transform * & ? into the appropriate regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the query into words, phrases, and operators\n",
    "def tokenize_query(query):\n",
    "    # Define regex patterns for various token types\n",
    "    token_patterns = [\n",
    "        r'\\(',  # Parenthesis\n",
    "        r'\\)',  # Parenthesis\n",
    "        r'\"[^\"]+\"',  # Quoted strings\n",
    "        r'[^) ]+', # everything that's not a space or a right parenthesis, including operators and wildcards\n",
    "        r'\\s+',  # Whitespace (to be ignored)\n",
    "    ]\n",
    "\n",
    "    # Combine the patterns into one single pattern\n",
    "    combined_pattern = '|'.join(token_patterns)\n",
    "    \n",
    "    # Use finditer for better control (positioning and matching)\n",
    "    tokens = []\n",
    "    for match in re.finditer(combined_pattern, query):\n",
    "        token = match.group(0).strip().lower() # Get the matched text and normalize it\n",
    "        if token.strip():  # Ignore empty tokens\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "############################################\n",
    "# Transformation function\n",
    "############################################\n",
    "\n",
    "# Handle implicit OR (e.g., \"word1 word2\" should be interpreted as \"word1 OR word2\")\n",
    "# Also implicit AND before NOT unless the query starts with NOT\n",
    "def add_implicit_boolean(tokens):\n",
    "    processed_tokens = []\n",
    "    prev_was_term = False  # Tracks if the last token was a word/phrase\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == \"not\":\n",
    "            # If the previous token is a term, insert implicit AND before NOT\n",
    "            if prev_was_term:\n",
    "                processed_tokens.append(\"and\")  # Add implicit AND before NOT\n",
    "            prev_was_term = False  # Reset, as NOT is an operator\n",
    "        elif token not in {\"and\", \"or\", \"not\"} and token not in {\"(\", \")\"}:\n",
    "            if prev_was_term:\n",
    "                processed_tokens.append(\"or\")  # Insert implicit OR\n",
    "            prev_was_term = True  # Mark that the last token was a term (word/phrase) \n",
    "        elif token == \")\":  \n",
    "            prev_was_term = True  # Consider closing parentheses as valid preceding terms\n",
    "        else:\n",
    "            prev_was_term = False  # Reset if an operator or parenthesis is found\n",
    "\n",
    "        processed_tokens.append(token)\n",
    "\n",
    "    return processed_tokens  # Return the processed tokens\n",
    "\n",
    "# Returns full and partial matches of a token from document_words\n",
    "def word_matches(token, document_words, partial = False):\n",
    "    if \"*\" in token or \"?\" in token:  # Handle wildcards\n",
    "        regex = token.replace(\"*\", \".*\").replace(\"?\", \".\")  # Convert to regex\n",
    "        return any(re.match(regex, word) for word in document_words) \n",
    "    if partial: # if user chooses partial matches\n",
    "        # Escape only the period character\n",
    "        if \".\" in token:\n",
    "            token = token.replace(\".\", r\"\\.\")  # Escape the period by adding a backslash\n",
    "        # returns True for partial matches \n",
    "        return any(re.match(token, word) for word in document_words) \n",
    "    else: # if user chooses full matches\n",
    "        # remove punctuation to ensure exact matches\n",
    "        document_words = {re.sub(r\"[^\\w\\s]\", \"\", word) for word in document_words}  \n",
    "        return token in document_words   # Return True for exact matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation functions\n",
    "\n",
    "These functions evaluate the documents using boolean operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a Boolean operation\n",
    "def evaluate(operator, operand1=None, operand2=None):\n",
    "    if operator == \"not\":\n",
    "        return not operand1\n",
    "    if operator == \"and\":\n",
    "        return operand1 and operand2\n",
    "    if operator == \"or\":\n",
    "        return operand1 or operand2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions\n",
    "\n",
    "These functions test the processed query against a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes an operator from the stack and applies it to the operands.\n",
    "# Returns the updated operand stack.\n",
    "def apply_operator(operator_stack, bool_results_stack):\n",
    "    if not operator_stack:\n",
    "        return bool_results_stack  # No change if empty\n",
    "\n",
    "    operator = operator_stack.pop()\n",
    "\n",
    "    if operator == \"not\":\n",
    "        result = evaluate(operator, bool_results_stack.pop())\n",
    "\n",
    "    else:\n",
    "        operand2 = bool_results_stack.pop()\n",
    "        operand1 = bool_results_stack.pop()\n",
    "        result = evaluate(operator, operand1, operand2)\n",
    "\n",
    "    bool_results_stack.append(result)  # Push result back onto stack\n",
    "    return operator_stack, bool_results_stack  # Return updated stacks\n",
    "\n",
    "def process_query(query, document, partial = False):\n",
    "    tokens = tokenize_query(query)\n",
    "    tokens = add_implicit_boolean(tokens)\n",
    "    \n",
    "    bool_results_stack = []  # Stores boolean results\n",
    "    operator_stack = []  # Stores operators\n",
    "\n",
    "    # creating the operator stack while respecting the order of operations\n",
    "    # first everything in parenthesis, then NOT, then AND/OR\n",
    "    # reassigning operator_stack and bool_results_stack for clarity but it's not necessary\n",
    "    for token in tokens:\n",
    "        if token == \"(\":\n",
    "            operator_stack.append(token)\n",
    "        elif token == \")\":\n",
    "            # evaluate everything in parenthesis\n",
    "            while operator_stack and operator_stack[-1] != \"(\":\n",
    "                operator_stack, bool_results_stack = apply_operator(operator_stack, bool_results_stack) \n",
    "            if operator_stack:\n",
    "                operator_stack.pop()  # Remove \"(\"\n",
    "        elif token == \"not\":\n",
    "            operator_stack.append(token)\n",
    "        elif token in {\"and\", \"or\"}:\n",
    "            # apply not first\n",
    "            while operator_stack and operator_stack[-1] == \"not\":\n",
    "                operator_stack, bool_results_stack = apply_operator(operator_stack, bool_results_stack)\n",
    "            # apply AND/OR next from left to right\n",
    "            while operator_stack and operator_stack[-1] in {\"and\", \"or\"}:\n",
    "                operator_stack, bool_results_stack = apply_operator(operator_stack, bool_results_stack)\n",
    "            operator_stack.append(token)\n",
    "        else:\n",
    "            if token.startswith('\"') and token.endswith('\"'):  \n",
    "                phrase = token.strip('\"')\n",
    "                bool_results_stack.append(phrase in document)\n",
    "            else:\n",
    "                document_words = document.split()\n",
    "                bool_results_stack.append(word_matches(token, document_words, partial))\n",
    "\n",
    "    # evaluating the operator stack by resolving the rest of it\n",
    "    while operator_stack:\n",
    "        operator_stack, bool_results_stack = apply_operator(operator_stack, bool_results_stack)\n",
    "\n",
    "    return bool_results_stack.pop() if bool_results_stack else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and search all docs functions\n",
    "\n",
    "Cleans documents to remove punctuation while searching all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the query across all documents\n",
    "def search_documents(query, documents, partial = False):\n",
    "    results = []\n",
    "    for doc in documents:\n",
    "        if process_query(query, doc.lower(), partial):\n",
    "            results.append(doc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing/example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "documents = [\n",
    "    \"Michelangelo Buonarroti was an Italian sculptor, painter, and architect, best known for painting the Sistine Chapel ceiling.\",\n",
    "    \"Leonardo da Vinci was a polymath who painted the famous Mona Lisa and The Last Supper.\",\n",
    "    \"Donatello was a sculptor known for his bronze statue of David, the first free-standing nude male sculpture since antiquity.\",\n",
    "    \"Raphael was an Italian painter and architect of the High Renaissance, famous for his fresco The School of Athens.\",\n",
    "    \"Michelangelo sculpted the marble statue David, which became a symbol of Florentine strength and beauty.\",\n",
    "    \"Leonardo da Vinci filled notebooks with anatomical studies, engineering designs, and artistic sketches.\",\n",
    "    \"Donatello’s Gattamelata is one of the earliest Renaissance equestrian statues, depicting a military leader.\",\n",
    "    \"Raphael worked on the Vatican’s Stanza della Segnatura, producing masterpieces like The School of Athens.\",\n",
    "    \"Michelangelo designed the dome of St. Peter’s Basilica in Vatican City, one of the most iconic architectural feats of the Renaissance.\",\n",
    "    \"Leonardo da Vinci studied human anatomy by dissecting corpses to improve his artistic accuracy.\",\n",
    "    \"Donatello pioneered the use of linear perspective in sculpture, particularly in his relief work The Feast of Herod.\",\n",
    "    \"Raphael was known for his harmonious and balanced compositions, influenced by both Leonardo and Michelangelo.\",\n",
    "    \"Michelangelo’s Pietà, housed in St. Peter’s Basilica, is a masterpiece of Renaissance sculpture depicting the Virgin Mary holding Jesus.\",\n",
    "    \"Leonardo da Vinci’s Vitruvian Man illustrates ideal human proportions based on the writings of the ancient Roman architect Vitruvius.\",\n",
    "    \"Donatello revived the lost-wax casting technique in bronze sculpture, making his works more detailed and expressive.\",\n",
    "    \"Raphael’s Sistine Madonna features the famous cherubs that have been widely reproduced in modern culture.\",\n",
    "    \"Michelangelo considered himself primarily a sculptor, even though he was also a renowned painter and architect.\",\n",
    "    \"Leonardo da Vinci developed early concepts for machines like the helicopter and parachute, centuries ahead of their time.\",\n",
    "    \"Donatello’s St. George statue in Florence showcases his skill in creating dynamic, lifelike figures.\",\n",
    "    \"Raphael’s premature death at 37 was deeply mourned, and he was buried in the Pantheon in Rome.\",\n",
    "    \"Titian was a Venetian Renaissance painter known for his use of vibrant colors and dynamic compositions, as seen in Assumption of the Virgin.\",\n",
    "    \"Sandro Botticelli painted The Birth of Venus, which depicts the goddess emerging from the sea on a shell.\",\n",
    "    \"Albrecht Dürer was a German Renaissance artist famous for his woodcuts, engravings, and self-portraits.\",\n",
    "    \"Jan van Eyck, a Flemish painter, pioneered the use of oil paints in works like The Arnolfini Portrait.\",\n",
    "    \"Hieronymus Bosch created surreal, fantastical paintings, including The Garden of Earthly Delights.\",\n",
    "    \"Giorgione, a Venetian painter, is best known for his mysterious painting The Tempest.\",\n",
    "    \"Masaccio, an early Renaissance painter, applied linear perspective in The Holy Trinity, making it one of the first paintings with true depth.\",\n",
    "    \"Paolo Veronese created grand, elaborate compositions, such as The Wedding at Cana, which is displayed in the Louvre.\",\n",
    "    \"Caravaggio, though from the later Baroque period, was heavily influenced by Renaissance techniques, particularly chiaroscuro.\",\n",
    "    \"Pieter Bruegel the Elder specialized in detailed depictions of peasant life, as seen in The Peasant Wedding.\",\n",
    "    \"Andrea Mantegna’s Lamentation of Christ is famous for its extreme use of foreshortening.\",\n",
    "    \"Fra Angelico was a Dominican friar known for painting religious frescoes, including The Annunciation.\",\n",
    "    \"Piero della Francesca’s The Flagellation of Christ is admired for its precise use of perspective.\",\n",
    "    \"Filippo Brunelleschi, an architect, engineered the massive dome of Florence’s Cathedral, a masterpiece of Renaissance architecture.\",\n",
    "    \"Lorenzo Ghiberti designed the bronze Gates of Paradise for the Baptistery of Florence.\",\n",
    "    \"Domenico Ghirlandaio was Michelangelo’s teacher and painted The Adoration of the Magi.\",\n",
    "    \"Benvenuto Cellini was a sculptor and goldsmith, best known for Perseus with the Head of Medusa.\",\n",
    "    \"Giovanni Bellini was a Venetian painter who contributed to the development of rich color techniques in oil painting.\",\n",
    "    \"Correggio’s Assumption of the Virgin uses dramatic foreshortening to create a dynamic, swirling composition.\",\n",
    "    \"Tintoretto’s The Last Supper reimagined the biblical scene with dramatic lighting and motion, differing from Leonardo’s version.\",\n",
    "    \"The Teenage Mutant Ninja Turtles (TMNT) were created by Kevin Eastman and Peter Laird in 1984 as a comic book series.\",\n",
    "    \"The four turtles—Leonardo, Michelangelo, Donatello, and Raphael—are named after famous Renaissance artists.\",\n",
    "    \"Splinter, their sensei and father figure, is a mutant rat who teaches them ninjutsu.\",\n",
    "    \"The turtles’ main enemy is the Shredder, the leader of the Foot Clan.\",\n",
    "    \"TMNT originally started as a dark and gritty comic but was later adapted into a more family-friendly animated series in 1987.\",\n",
    "    \"The turtles live in the sewers of New York City and fight crime while staying hidden from society.\",\n",
    "    \"April O’Neil, a journalist and ally of the turtles, often helps them in their battles against villains.\",\n",
    "    \"Each turtle has a signature weapon: Leonardo wields katanas, Michelangelo uses nunchaku, Donatello fights with a bo staff, and Raphael has twin sai.\",\n",
    "    \"Michelangelo is known for his love of pizza and his laid-back, fun-loving personality.\",\n",
    "    \"The phrase Cowabunga! became one of the franchise’s most famous catchphrases, often said by Michelangelo.\",\n",
    "    \"TMNT has been adapted into multiple animated series, live-action films, and video games over the decades.\",\n",
    "    \"In the 1990 live-action Teenage Mutant Ninja Turtles movie, the turtles’ suits were created by Jim Henson’s Creature Shop.\",\n",
    "    \"The 2003 animated series returned to a darker tone, closer to the original comics.\",\n",
    "    \"The turtles' main ally in their fight against evil is Casey Jones, a hockey mask-wearing vigilante.\",\n",
    "    \"Over the years, TMNT has remained a pop culture phenomenon, with new adaptations and reboots continuing to introduce the turtles to new generations.\",\n",
    "    \"Leonardo is the leader of the Teenage Mutant Ninja Turtles and is known for his discipline and mastery of the katana swords.\",\n",
    "    \"Michelangelo is the most fun-loving turtle, often cracking jokes and obsessing over pizza.\",\n",
    "    \"Donatello is the team’s tech expert, constantly inventing gadgets and using his bo staff in battle.\",\n",
    "    \"Raphael is the most aggressive and rebellious turtle, often clashing with Leonardo over leadership.\",\n",
    "    \"Leonardo wears a blue mask and is deeply dedicated to following Splinter’s teachings.\",\n",
    "    \"Michelangelo is the turtle who most frequently shouts “Cowabunga!” when excited.\",\n",
    "    \"Donatello created the Turtle Van, a high-tech vehicle that helps the team travel through New York City.\",\n",
    "    \"Raphael has a close friendship with Casey Jones, often teaming up with him in fights.\",\n",
    "    \"Leonardo was once brainwashed by Shredder in some versions of the TMNT storylines.\",\n",
    "    \"Michelangelo is often underestimated but has proven to be a skilled fighter when necessary.\",\n",
    "    \"Leonardo is the disciplined leader of the Ninja Turtles, always striving to keep the team focused on their mission.\",\n",
    "    \"Michelangelo and Donatello have a playful dynamic, with Mikey often testing Donnie’s patience with his goofy antics.\",\n",
    "    \"Raphael, known for his temper, frequently clashes with Leonardo over leadership decisions.\",\n",
    "    \"Donatello is the team’s tech genius, constantly inventing new gadgets to help Michelangelo, Raphael, and Leonardo in battle.\",\n",
    "    \"Michelangelo is the turtle who loves pizza the most, though Raphael and Donatello don’t mind grabbing a slice.\",\n",
    "    \"Leonardo and Raphael have a sibling rivalry, but deep down, they respect and protect each other.\",\n",
    "    \"Donatello once built a special armored vehicle, the Shellraiser, for Leonardo, Michelangelo, and Raphael to use against the Foot Clan.\",\n",
    "    \"Raphael is the toughest fighter of the group, but even he admits that Leonardo has the best strategy in battle.\",\n",
    "    \"Michelangelo, Donatello, and Raphael sometimes tease Leonardo for being too serious, but they always follow his lead in tough situations.\",\n",
    "    \"While Michelangelo loves to joke around, Leonardo, Donatello, and Raphael know they can always count on him when the fight gets serious.\",\n",
    "    \"Turtles have been around for over 200 million years, making them one of the oldest reptile groups still roaming the planet.\",\n",
    "    \"A turtle’s shell isn’t just armor—it’s part of its skeleton, so taking it off would be like removing its ribs!\",\n",
    "    \"Some turtles, like the leatherback sea turtle, are world travelers, crossing entire oceans without a GPS.\",\n",
    "    \"Turtles may look slow on land, but put them in water, and many become graceful, speedy swimmers.\",\n",
    "    \"A tortoise might take its time, but don’t mistake that for laziness—it’s all about patience and wisdom.\",\n",
    "    \"Sea turtles can hold their breath for hours, making them the ultimate underwater meditation masters.\",\n",
    "    \"If a turtle could talk, it might brag about its long lifespan—some live over 150 years!\",\n",
    "    \"Many turtles have excellent night vision, perfect for sneaking around in the moonlight like little reptilian ninjas.\",\n",
    "    \"Unlike humans, turtles don’t have vocal cords, but that doesn’t stop them from making little grunts, hisses, and even some underwater sounds.\",\n",
    "    \"The alligator snapping turtle looks like a prehistoric warrior, with its spiky shell and powerful beak.\",\n",
    "    \"Baby sea turtles hatch from their eggs and immediately embark on an epic race to the ocean, dodging hungry predators along the way.\",\n",
    "    \"Some turtles, like the red-eared slider, love basking in the sun, soaking up rays like tiny, sun-worshipping yogis.\",\n",
    "    \"The painted turtle has antifreeze-like blood, allowing it to survive in icy waters during winter—talk about cold-blooded resilience!\",\n",
    "    \"A turtle’s sense of smell is incredible, helping it sniff out food, friends, and danger from surprising distances.\",\n",
    "    \"While turtles don’t rush through life, their steady and determined nature has made them symbols of wisdom, perseverance, and patience across many cultures.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is where we do the search. Feel free to change the query in teh single quotes below to see what kind of searches you can do with the documents!\n",
    "\n",
    "For example: \n",
    "\n",
    "- Can you just get articles on Michelangelo the painter? \n",
    "- Or facts about turtles without getting documents on the Teenage Mutant Ninja Turtles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = 'renaissance Michelangelo \"italian sculptor\"'\n",
    "\n",
    "# Perform the search\n",
    "results = search_documents(query, documents, partial = False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Documents matching the query:\")\n",
    "for result in results:\n",
    "    print(\"-\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ranking / Relevance examples\n",
    "\n",
    "This section is going to open up a whole can of worms.\n",
    "- Do we want to get partial or full matches? \n",
    "- How do we handle wildcards and truncation?\n",
    "- Do we want to rank the results we get or do a new search?\n",
    "- At what point do we cap results?\n",
    "- How can we compare the relevance algorithms?\n",
    "\n",
    "I made some choices to make this easier, namely yes to phrases and avoiding truncation, wildcards, and parenthesis for now. We'll need to use new queries.\n",
    "\n",
    "---\n",
    "\n",
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(query):\n",
    "    # find all phrases\n",
    "    phrases = re.findall(r'\"([^\"]*)\"', query)\n",
    "        \n",
    "    return phrases  \n",
    "\n",
    "# Tokenize and preprocess documents (remove punctuation and lowercasing)\n",
    "def preprocess(query, doc = None):\n",
    "    # Step 1: Extract phrases from the document\n",
    "    phrases = get_phrases(query)  # Get all quoted phrases\n",
    "\n",
    "    if doc:\n",
    "        # Step 2: Replace phrases in the document with placeholders so we don't tokenize them\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            doc = doc.lower().replace(f'{phrase}', f'__phrase{i}__')  # Replace phrases with unique placeholders\n",
    "\n",
    "        # Step 3: Tokenize the rest of the document (after replacing the phrases)\n",
    "        tokens = [word.lower() for word in doc.split(' ')]\n",
    "\n",
    "        # Step 4: Replace the placeholders with the actual phrases\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"__phrase\"):  # If it's a placeholder for a phrase\n",
    "                phrase_index = int(token.split('phrase')[1].split('__')[0])  # Get the index of the phrase\n",
    "                final_tokens.append(phrases[phrase_index])  # Replace placeholder with actual phrase\n",
    "            else:\n",
    "                final_tokens.append(token)\n",
    "\n",
    "        return final_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Your Query\n",
    "\n",
    "Again, feel free to change this to try different searches. Are there any surprises in the rankings? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'renaissance Michelangelo \"italian sculptor\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Set Similarity\n",
    "\n",
    "This divides the tokens in both the query set and document set (the intersection) with the total number of tokens in the query set and document set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Jaccard Similarity \n",
    "# incorporating word_matches for partial and wildcard matching\n",
    "def jaccard_similarity(query_set, doc_set):\n",
    "    # Calculate the intersection using word_matches for partial matching\n",
    "    # This is the boolean AND\n",
    "    intersection = sum(1 for term in query_set if \n",
    "                       any(word_matches(term, doc_set) \n",
    "                           for word in doc_set))\n",
    "\n",
    "    # Calculate the union using word_matches for partial matching\n",
    "    # This is the boolean OR\n",
    "    union = len(query_set) + len(doc_set) - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0  # Avoid division by zero\n",
    "\n",
    "# Convert query and documents into sets of terms (tokens)\n",
    "query_set = tokenize_query(query)\n",
    "\n",
    "# Replace all double quotes in the extracted phrases\n",
    "query_set = [phrase.replace('\"', '') for phrase in query_set]\n",
    "\n",
    "# List to store Jaccard similarity scores for each document\n",
    "similarity_scores = []\n",
    "\n",
    "# Compute Jaccard similarity for each document\n",
    "for doc in documents:\n",
    "    doc_set = preprocess(query, doc)  # Convert document to a set of terms\n",
    "    score = jaccard_similarity(query_set, doc_set)\n",
    "    similarity_scores.append(score)\n",
    "\n",
    "# Combine documents with their similarity scores\n",
    "doc_scores = list(zip(documents, similarity_scores))\n",
    "\n",
    "# Sort documents by their similarity score in descending order\n",
    "sorted_doc_scores = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Prepare data for Jaccard ranking\n",
    "jaccard_data = {\n",
    "    \"Document\": [],\n",
    "    \"Token Length\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Similarity\": []\n",
    "}\n",
    "\n",
    "for idx, (doc, score) in enumerate(sorted_doc_scores, start=1):\n",
    "    if idx > 26:  # Limit to the top 10 results\n",
    "        break\n",
    "    jaccard_data[\"Rank\"].append(idx)\n",
    "    jaccard_data[\"Document\"].append(doc)\n",
    "    jaccard_data[\"Similarity\"].append(f\"{score:.4}\")\n",
    "    jaccard_data[\"Token Length\"].append(len(preprocess(query, doc)))\n",
    "\n",
    "# Create pandas DataFrame for Jaccard results\n",
    "jaccard_df = pd.DataFrame(jaccard_data)\n",
    "\n",
    "jaccard_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency. It penalizes the term frequency against the number of documents a term is in. \n",
    "\n",
    "In the query: **renaissance Michalangelo \"italian sculptor\"** each document only has a phrase appear once. The document frequency however is\n",
    "- Michelangelo = 29\n",
    "- Renaissance = 16\n",
    "- italian sculptor = 1\n",
    "\n",
    "Michelangelo will penalized the most and 'italian sculptor' the least. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get phrases from the query\n",
    "phrases = get_phrases(query)\n",
    "\n",
    "for i, phrase in enumerate(phrases):\n",
    "    query_processed = query.lower().replace(phrase, \"__phrase__\" + str(i)).replace('\"', '')\n",
    "\n",
    "# Create a new list of processed documents\n",
    "documents_processed = []\n",
    "for doc in documents:\n",
    "    processed_doc = doc.lower()\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        processed_doc = processed_doc.replace(phrase.lower(), \"__phrase__\" + str(i))\n",
    "    documents_processed.append(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(documents_processed)\n",
    "\n",
    "# Transform the query into a TF-IDF vector (using the same vectorizer)\n",
    "query_vector = vectorizer.transform([query_processed])\n",
    "\n",
    "# Compute cosine similarity between the query and all documents\n",
    "similarity_scores = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
    "\n",
    "# Rank documents based on similarity scores\n",
    "tfidf_ranked_indices = np.argsort(similarity_scores)[::-1]  # Sort in descending order\n",
    "tfidf_ranked_scores = similarity_scores[tfidf_ranked_indices]\n",
    "\n",
    "# Prepare data for TF-IDF ranking\n",
    "tfidf_data = {\n",
    "    \"Rank\": [],\n",
    "    \"Score\": [],\n",
    "    \"Document\": []    \n",
    "}\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(tfidf_ranked_indices, tfidf_ranked_scores)):\n",
    "    if i >= 25:  # Limit to the top 25 results\n",
    "        break\n",
    "    tfidf_data[\"Rank\"].append(i+1)\n",
    "    tfidf_data[\"Score\"].append(f\"{score:.4f}\")\n",
    "    tfidf_data[\"Document\"].append(documents[idx])\n",
    "\n",
    "# Create pandas DataFrame for TF-IDF results\n",
    "tfidf_df = pd.DataFrame(tfidf_data)\n",
    "\n",
    "tfidf_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25\n",
    "\n",
    "BM25 takes TF-IDF and normalizes based on document length. It's a probabilistic model and has parameters that can be tuned.\n",
    "\n",
    "In the query: **renaissance Michalangelo \"italian sculptor\"** each document only has a phrase appear once. The document frequency however is\n",
    "- Michelangelo = 29\n",
    "- Renaissance = 16\n",
    "- italian sculptor = 1\n",
    "\n",
    "The minimum number of tokens is 11 and the maximum is 23. The average is 16.7 tokens.\n",
    "\n",
    "Michelangelo will be penalized more than Renaissance but if either term appears in a short document then it might score higher than documents with two terms. \n",
    "\n",
    "This is why the below is going to have different results than TF-IDF. Rarity will matter less if the document length is average or high. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Tokenize the processed documents\n",
    "tokenized_documents = [doc.split(\" \") for doc in documents_processed]\n",
    "\n",
    "# tokenize processed query\n",
    "tokenized_query = query_processed.split(\" \")\n",
    "\n",
    "# Initialize the BM25 model\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "####################################################################\n",
    "# Set parameters for BM25 Okapi\n",
    "# These are defaults\n",
    "\n",
    "bm25.k1 = 1.5  # Term frequency saturation\n",
    "# A higher k1 means that it's more sensitive to \n",
    "# term frequencies. \n",
    "\n",
    "bm25.b = 0.75  # Length normalization\n",
    "# A higher b means that it normalizes less for document length, \n",
    "# meaning longer documents will be less penalized.\n",
    "#####################################################################\n",
    "\n",
    "# Get the BM25 scores for each document based on the query\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Retrieve the documents with the highest BM25 scores\n",
    "bm25_ranked_docs = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Prepare data for BM25 ranking\n",
    "bm25_data = {\n",
    "    \"Document\": [],\n",
    "    \"Length\": [],\n",
    "    \"Rank\": [],\n",
    "    \"Score\": [],\n",
    "}\n",
    "\n",
    "counter = 1\n",
    "for idx, score in bm25_ranked_docs:\n",
    "    if counter > 25:  # Limit to the top 25 results\n",
    "        break\n",
    "    bm25_data[\"Rank\"].append(counter)\n",
    "    bm25_data[\"Score\"].append(f\"{score:.4f}\")\n",
    "    bm25_data[\"Document\"].append(documents[idx])\n",
    "    bm25_data[\"Length\"].append(len(documents_processed[idx].split()))\n",
    "    counter += 1\n",
    "\n",
    "# Create pandas DataFrame for BM25 results\n",
    "bm25_df = pd.DataFrame(bm25_data)\n",
    "\n",
    "bm25_df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - different algorithm choices lead to different rankings\n",
    "\n",
    "These numbers are manipulatable and there are entire fields like Search Engine Optimization designed to get to the top of certain rankings and not another. How does Jaccard, TF-IDF, and BM25 rank things differently from your search? Why might you choose one and not another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns for clarity\n",
    "jaccard_df = jaccard_df.rename(columns={\"Rank\": \"Jaccard Rank\", \"Similarity\": \"Jaccard Similarity\"})\n",
    "tfidf_df = tfidf_df.rename(columns={\"Rank\": \"TF-IDF Rank\", \"Score\": \"TF-IDF Score\"})\n",
    "bm25_df = bm25_df.rename(columns={\"Rank\": \"BM25 Rank\", \"Score\": \"BM25 Score\", \"Length\": \"Token Length\"})\n",
    "\n",
    "# Merge the DataFrames on the 'Document' column\n",
    "merged_df = pd.merge(jaccard_df, tfidf_df, on=\"Document\", how=\"outer\")\n",
    "\n",
    "merged_df.head(10)\n",
    "\n",
    "merged_df = pd.merge(merged_df, bm25_df, on=[\"Document\", \"Token Length\"], how=\"outer\")\n",
    "\n",
    "print(\"Results for query:\", query) \n",
    "\n",
    "# Print the merged DataFrame\n",
    "merged_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
